{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import (\n",
    "    LabelEncoder, MinMaxScaler\n",
    "    )\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, roc_auc_score, f1_score, confusion_matrix, accuracy_score\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the classifiers, let's find all the estimators that have probabilities as part of their prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eikegermann/anaconda3/envs/earthquake36/lib/python3.6/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier\n",
      "BaggingClassifier\n",
      "BayesianGaussianMixture\n",
      "BernoulliNB\n",
      "CalibratedClassifierCV\n",
      "CategoricalNB\n",
      "ClassifierChain\n",
      "ComplementNB\n",
      "DecisionTreeClassifier\n",
      "DummyClassifier\n",
      "ExtraTreeClassifier\n",
      "ExtraTreesClassifier\n",
      "GaussianMixture\n",
      "GaussianNB\n",
      "GaussianProcessClassifier\n",
      "GradientBoostingClassifier\n",
      "GridSearchCV\n",
      "HistGradientBoostingClassifier\n",
      "KNeighborsClassifier\n",
      "LabelPropagation\n",
      "LabelSpreading\n",
      "LinearDiscriminantAnalysis\n",
      "LogisticRegression\n",
      "LogisticRegressionCV\n",
      "MLPClassifier\n",
      "MultiOutputClassifier\n",
      "MultinomialNB\n",
      "NoSampleWeightWrapper\n",
      "NuSVC\n",
      "OneVsRestClassifier\n",
      "Pipeline\n",
      "QuadraticDiscriminantAnalysis\n",
      "RFE\n",
      "RFECV\n",
      "RadiusNeighborsClassifier\n",
      "RandomForestClassifier\n",
      "RandomizedSearchCV\n",
      "SGDClassifier\n",
      "SVC\n",
      "StackingClassifier\n",
      "VotingClassifier\n",
      "_BinaryGaussianProcessClassifierLaplace\n",
      "_ConstantPredictor\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.testing import all_estimators\n",
    "\n",
    "estimators = all_estimators()\n",
    "\n",
    "for name, class_ in estimators:\n",
    "    if hasattr(class_, 'predict_proba'):\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, not all of those are classifiers. But good enough. We'll test on all that are interesting for our case.  \n",
    "Special case: Classifiers with feature importances for specific methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier\n",
      "BernoulliNB\n",
      "CategoricalNB\n",
      "ComplementNB\n",
      "DecisionTreeClassifier\n",
      "ExtraTreeClassifier\n",
      "ExtraTreesClassifier\n",
      "GradientBoostingClassifier\n",
      "MultinomialNB\n",
      "NuSVC\n",
      "OneVsRestClassifier\n",
      "RandomForestClassifier\n",
      "SVC\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.testing import all_estimators\n",
    "\n",
    "estimators = all_estimators()\n",
    "\n",
    "for name, class_ in estimators:\n",
    "    if hasattr(class_, 'predict_proba') and (hasattr(class_, 'coef_') or hasattr(class_, 'feature_importances_')):\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's collect all those estimators into one dictionary for easy access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_dict = dict(estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "quake_frame = pd.read_csv('data/consolidated_data.csv')\n",
    "\n",
    "quake_frame['simple_label'] = quake_frame['type'] != 'earthquake'\n",
    "\n",
    "quake_frame.drop(['id', 'Unnamed: 0', 'place', 'time', 'updated', 'type'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No imputation\n",
    "\n",
    "We throw away the NaN values and encode the data like the other baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "latitude           0\n",
       "longitude          0\n",
       "depth              0\n",
       "mag                0\n",
       "magType            0\n",
       "nst                0\n",
       "gap                0\n",
       "dmin               0\n",
       "rms                0\n",
       "net                0\n",
       "horizontalError    0\n",
       "depthError         0\n",
       "magError           0\n",
       "magNst             0\n",
       "status             0\n",
       "locationSource     0\n",
       "magSource          0\n",
       "simple_label       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quake_frame.dropna(inplace=True)\n",
    "quake_frame.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1227408"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(quake_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>latitude</th>\n",
       "      <td>1227408.0</td>\n",
       "      <td>37.361674</td>\n",
       "      <td>4.841731</td>\n",
       "      <td>0.000</td>\n",
       "      <td>35.964167</td>\n",
       "      <td>37.573000</td>\n",
       "      <td>38.817000</td>\n",
       "      <td>62.030667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>longitude</th>\n",
       "      <td>1227408.0</td>\n",
       "      <td>-119.557707</td>\n",
       "      <td>10.027502</td>\n",
       "      <td>-179.098</td>\n",
       "      <td>-122.701333</td>\n",
       "      <td>-120.558833</td>\n",
       "      <td>-118.150167</td>\n",
       "      <td>179.661500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depth</th>\n",
       "      <td>1227408.0</td>\n",
       "      <td>6.016756</td>\n",
       "      <td>7.922880</td>\n",
       "      <td>-3.882</td>\n",
       "      <td>1.816000</td>\n",
       "      <td>4.413000</td>\n",
       "      <td>7.830000</td>\n",
       "      <td>211.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mag</th>\n",
       "      <td>1227408.0</td>\n",
       "      <td>1.258097</td>\n",
       "      <td>0.694405</td>\n",
       "      <td>-2.500</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.180000</td>\n",
       "      <td>1.670000</td>\n",
       "      <td>5.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nst</th>\n",
       "      <td>1227408.0</td>\n",
       "      <td>17.010182</td>\n",
       "      <td>13.671235</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>276.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gap</th>\n",
       "      <td>1227408.0</td>\n",
       "      <td>121.032150</td>\n",
       "      <td>65.767724</td>\n",
       "      <td>0.000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>360.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmin</th>\n",
       "      <td>1227408.0</td>\n",
       "      <td>0.078264</td>\n",
       "      <td>0.342578</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.017120</td>\n",
       "      <td>0.037840</td>\n",
       "      <td>0.079990</td>\n",
       "      <td>141.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rms</th>\n",
       "      <td>1227408.0</td>\n",
       "      <td>0.097118</td>\n",
       "      <td>0.195847</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>64.290000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>horizontalError</th>\n",
       "      <td>1227408.0</td>\n",
       "      <td>0.801039</td>\n",
       "      <td>2.296862</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>194.584100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depthError</th>\n",
       "      <td>1227408.0</td>\n",
       "      <td>2.773763</td>\n",
       "      <td>6.903563</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>1.460000</td>\n",
       "      <td>725.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>magError</th>\n",
       "      <td>1227408.0</td>\n",
       "      <td>0.158215</td>\n",
       "      <td>0.132854</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>6.110000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>magNst</th>\n",
       "      <td>1227408.0</td>\n",
       "      <td>12.685281</td>\n",
       "      <td>17.846407</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>430.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     count        mean        std      min         25%  \\\n",
       "latitude         1227408.0   37.361674   4.841731    0.000   35.964167   \n",
       "longitude        1227408.0 -119.557707  10.027502 -179.098 -122.701333   \n",
       "depth            1227408.0    6.016756   7.922880   -3.882    1.816000   \n",
       "mag              1227408.0    1.258097   0.694405   -2.500    0.800000   \n",
       "nst              1227408.0   17.010182  13.671235    0.000    8.000000   \n",
       "gap              1227408.0  121.032150  65.767724    0.000   72.000000   \n",
       "dmin             1227408.0    0.078264   0.342578    0.000    0.017120   \n",
       "rms              1227408.0    0.097118   0.195847    0.000    0.030000   \n",
       "horizontalError  1227408.0    0.801039   2.296862    0.000    0.270000   \n",
       "depthError       1227408.0    2.773763   6.903563    0.000    0.490000   \n",
       "magError         1227408.0    0.158215   0.132854    0.000    0.090000   \n",
       "magNst           1227408.0   12.685281  17.846407    0.000    4.000000   \n",
       "\n",
       "                        50%         75%         max  \n",
       "latitude          37.573000   38.817000   62.030667  \n",
       "longitude       -120.558833 -118.150167  179.661500  \n",
       "depth              4.413000    7.830000  211.000000  \n",
       "mag                1.180000    1.670000    5.840000  \n",
       "nst               13.000000   22.000000  276.000000  \n",
       "gap              105.000000  153.000000  360.000000  \n",
       "dmin               0.037840    0.079990  141.160000  \n",
       "rms                0.060000    0.130000   64.290000  \n",
       "horizontalError    0.410000    0.720000  194.584100  \n",
       "depthError         0.770000    1.460000  725.300000  \n",
       "magError           0.140000    0.200000    6.110000  \n",
       "magNst             8.000000   14.000000  430.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quake_frame.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, this changes the proportions slightly, but not too bad. If anything, one might suggest that at least the mild increase in proportion of non-earthquakes offsets the reduced dataset a little.  \n",
    "Okay, so the problematic values are no longer there, that's something.  \n",
    "Let's try this.  \n",
    "We'll start by mixing up the data frame, then encoding all the categories numerically and splitting it sklearn style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "quake_frame = quake_frame.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "cat_columns = ['magType', 'net', 'status', 'locationSource', 'magSource']\n",
    "\n",
    "for cat in cat_columns:\n",
    "    quake_frame = pd.concat([quake_frame,\n",
    "                             pd.get_dummies(quake_frame[cat], prefix=cat)],\n",
    "                            axis=1)\n",
    "\n",
    "scale_cols = ['latitude', 'longitude', 'depth', 'mag', 'nst', 'gap', 'dmin', 'rms', 'horizontalError',\n",
    " 'depthError', 'magError', 'magNst']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "quake_frame[scale_cols] = scaler.fit_transform(quake_frame[scale_cols])\n",
    "\n",
    "x_cols = ['latitude', 'longitude', 'depth', 'mag', 'nst', 'gap', 'dmin', 'rms', 'horizontalError', 'depthError',\n",
    " 'magError', 'magNst', 'magType_Mb', 'magType_Md', 'magType_Ml', 'magType_Unknown', 'magType_ma', 'magType_mb',\n",
    " 'magType_mc', 'magType_md', 'magType_me', 'magType_mh', 'magType_ml', 'magType_mlg', 'magType_mlr', 'magType_mw',\n",
    " 'net_av', 'net_ci', 'net_hv', 'net_ismpkansas', 'net_ld', 'net_mb', 'net_nc', 'net_nm', 'net_nn', 'net_pr',\n",
    " 'net_se', 'net_uu', 'net_uw', 'status_automatic', 'status_manual', 'status_reviewed', 'locationSource_av',\n",
    " 'locationSource_ci', 'locationSource_hv', 'locationSource_ismp', 'locationSource_ld', 'locationSource_mb',\n",
    " 'locationSource_nc', 'locationSource_nm', 'locationSource_nn', 'locationSource_pr', 'locationSource_se',\n",
    " 'locationSource_uu', 'locationSource_uw', 'magSource_av', 'magSource_ci', 'magSource_hv', 'magSource_ismp',\n",
    " 'magSource_ld', 'magSource_mb', 'magSource_nc', 'magSource_nm', 'magSource_nn', 'magSource_pr', 'magSource_se',\n",
    " 'magSource_uu', 'magSource_uw']\n",
    "\n",
    "y_col = ['simple_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_length = int(np.round(len(quake_frame.index) * 0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = quake_frame.loc[:train_length, x_cols]\n",
    "train_y = quake_frame.loc[:train_length, y_col]\n",
    "\n",
    "valid_X = quake_frame.loc[train_length:, x_cols]\n",
    "valid_y = quake_frame.loc[train_length:, y_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = estimator_dict['AdaBoostClassifier'](random_state=42)\n",
    "\n",
    "clf.fit(train_X, np.ravel(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.7889921372408863\n",
      "Recall:  0.6336394948335247\n",
      "ROC score:  0.81370282480767\n",
      "F1 score:  0.7028334925183063\n",
      "Accuracy score:  0.9809884227764154\n"
     ]
    }
   ],
   "source": [
    "preds = pd.DataFrame(clf.predict(valid_X), columns=['predictions'])\n",
    "\n",
    "prec = precision_score(valid_y, preds)\n",
    "reca = recall_score(valid_y, preds)\n",
    "roc = roc_auc_score(valid_y, preds)\n",
    "f1 = f1_score(valid_y, preds)\n",
    "acc = accuracy_score(valid_y, preds)\n",
    "conf_mat = confusion_matrix(valid_y, preds)\n",
    "\n",
    "print(\"Precision: \", prec)\n",
    "print(\"Recall: \", reca)\n",
    "print(\"ROC score: \", roc)\n",
    "print(\"F1 score: \", f1)\n",
    "print(\"Accuracy score: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.9568292682926829\n",
      "Recall:  0.9008036739380023\n",
      "ROC score:  0.9496542823595033\n",
      "F1 score:  0.9279716144293317\n",
      "Accuracy score:  0.995038332749448\n"
     ]
    }
   ],
   "source": [
    "clf = estimator_dict['BaggingClassifier'](random_state=42)\n",
    "\n",
    "clf.fit(train_X, np.ravel(train_y))\n",
    "\n",
    "preds = pd.DataFrame(clf.predict(valid_X), columns=['predictions'])\n",
    "\n",
    "prec = precision_score(valid_y, preds)\n",
    "reca = recall_score(valid_y, preds)\n",
    "roc = roc_auc_score(valid_y, preds)\n",
    "f1 = f1_score(valid_y, preds)\n",
    "acc = accuracy_score(valid_y, preds)\n",
    "conf_mat = confusion_matrix(valid_y, preds)\n",
    "\n",
    "print(\"Precision: \", prec)\n",
    "print(\"Recall: \", reca)\n",
    "print(\"ROC score: \", roc)\n",
    "print(\"F1 score: \", f1)\n",
    "print(\"Accuracy score: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try BayesianGaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eikegermann/anaconda3/envs/earthquake37/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.0\n",
      "Recall:  0.0\n",
      "ROC score:  0.5\n",
      "F1 score:  0.0\n",
      "Accuracy score:  0.9645187834545914\n"
     ]
    }
   ],
   "source": [
    "clf = estimator_dict['BayesianGaussianMixture'](random_state=42)\n",
    "\n",
    "clf.fit(train_X, np.ravel(train_y))\n",
    "\n",
    "preds = pd.DataFrame(clf.predict(valid_X), columns=['predictions'])\n",
    "\n",
    "prec = precision_score(valid_y, preds)\n",
    "reca = recall_score(valid_y, preds)\n",
    "roc = roc_auc_score(valid_y, preds)\n",
    "f1 = f1_score(valid_y, preds)\n",
    "acc = accuracy_score(valid_y, preds)\n",
    "conf_mat = confusion_matrix(valid_y, preds)\n",
    "\n",
    "print(\"Precision: \", prec)\n",
    "print(\"Recall: \", reca)\n",
    "print(\"ROC score: \", roc)\n",
    "print(\"F1 score: \", f1)\n",
    "print(\"Accuracy score: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.14831373311303334\n",
      "Recall:  0.3504018369690011\n",
      "ROC score:  0.6381906301058072\n",
      "F1 score:  0.20841300191204587\n",
      "Accuracy score:  0.9055572302653555\n"
     ]
    }
   ],
   "source": [
    "clf = estimator_dict['BernoulliNB']()\n",
    "\n",
    "clf.fit(train_X, np.ravel(train_y))\n",
    "\n",
    "preds = pd.DataFrame(clf.predict(valid_X), columns=['predictions'])\n",
    "\n",
    "prec = precision_score(valid_y, preds)\n",
    "reca = recall_score(valid_y, preds)\n",
    "roc = roc_auc_score(valid_y, preds)\n",
    "f1 = f1_score(valid_y, preds)\n",
    "acc = accuracy_score(valid_y, preds)\n",
    "conf_mat = confusion_matrix(valid_y, preds)\n",
    "\n",
    "print(\"Precision: \", prec)\n",
    "print(\"Recall: \", reca)\n",
    "print(\"ROC score: \", roc)\n",
    "print(\"F1 score: \", f1)\n",
    "print(\"Accuracy score: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try CalibratedClassifierCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.748072515107314\n",
      "Recall:  0.4121699196326062\n",
      "ROC score:  0.7035318707686116\n",
      "F1 score:  0.5314975201717373\n",
      "Accuracy score:  0.9742180689419183\n"
     ]
    }
   ],
   "source": [
    "clf = estimator_dict['CalibratedClassifierCV']()\n",
    "\n",
    "clf.fit(train_X, np.ravel(train_y))\n",
    "\n",
    "preds = pd.DataFrame(clf.predict(valid_X), columns=['predictions'])\n",
    "\n",
    "prec = precision_score(valid_y, preds)\n",
    "reca = recall_score(valid_y, preds)\n",
    "roc = roc_auc_score(valid_y, preds)\n",
    "f1 = f1_score(valid_y, preds)\n",
    "acc = accuracy_score(valid_y, preds)\n",
    "conf_mat = confusion_matrix(valid_y, preds)\n",
    "\n",
    "print(\"Precision: \", prec)\n",
    "print(\"Recall: \", reca)\n",
    "print(\"ROC score: \", roc)\n",
    "print(\"F1 score: \", f1)\n",
    "print(\"Accuracy score: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try ComplementNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.07144813607603605\n",
      "Recall:  0.5989667049368542\n",
      "ROC score:  0.6563055273877587\n",
      "F1 score:  0.12766738449490994\n",
      "Accuracy score:  0.7095754474869848\n"
     ]
    }
   ],
   "source": [
    "clf = estimator_dict['ComplementNB']()\n",
    "\n",
    "clf.fit(train_X, np.ravel(train_y))\n",
    "\n",
    "preds = pd.DataFrame(clf.predict(valid_X), columns=['predictions'])\n",
    "\n",
    "prec = precision_score(valid_y, preds)\n",
    "reca = recall_score(valid_y, preds)\n",
    "roc = roc_auc_score(valid_y, preds)\n",
    "f1 = f1_score(valid_y, preds)\n",
    "acc = accuracy_score(valid_y, preds)\n",
    "conf_mat = confusion_matrix(valid_y, preds)\n",
    "\n",
    "print(\"Precision: \", prec)\n",
    "print(\"Recall: \", reca)\n",
    "print(\"ROC score: \", roc)\n",
    "print(\"F1 score: \", f1)\n",
    "print(\"Accuracy score: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.8955750199067228\n",
      "Recall:  0.9039035591274397\n",
      "ROC score:  0.9500132057458697\n",
      "F1 score:  0.8997200159990858\n",
      "Accuracy score:  0.9928507996512983\n"
     ]
    }
   ],
   "source": [
    "clf = estimator_dict['DecisionTreeClassifier'](random_state=42)\n",
    "\n",
    "clf.fit(train_X, np.ravel(train_y))\n",
    "\n",
    "preds = pd.DataFrame(clf.predict(valid_X), columns=['predictions'])\n",
    "\n",
    "prec = precision_score(valid_y, preds)\n",
    "reca = recall_score(valid_y, preds)\n",
    "roc = roc_auc_score(valid_y, preds)\n",
    "f1 = f1_score(valid_y, preds)\n",
    "acc = accuracy_score(valid_y, preds)\n",
    "conf_mat = confusion_matrix(valid_y, preds)\n",
    "\n",
    "print(\"Precision: \", prec)\n",
    "print(\"Recall: \", reca)\n",
    "print(\"ROC score: \", roc)\n",
    "print(\"F1 score: \", f1)\n",
    "print(\"Accuracy score: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eikegermann/anaconda3/envs/earthquake37/lib/python3.7/site-packages/sklearn/dummy.py:132: FutureWarning: The default value of strategy will change from stratified to prior in 0.24.\n",
      "  \"stratified to prior in 0.24.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.03354779411764706\n",
      "Recall:  0.033524684270952926\n",
      "ROC score:  0.4989984173470724\n",
      "F1 score:  0.03353623521304698\n",
      "Accuracy score:  0.9314410017842448\n"
     ]
    }
   ],
   "source": [
    "clf = estimator_dict['DummyClassifier'](random_state=42)\n",
    "\n",
    "clf.fit(train_X, np.ravel(train_y))\n",
    "\n",
    "preds = pd.DataFrame(clf.predict(valid_X), columns=['predictions'])\n",
    "\n",
    "prec = precision_score(valid_y, preds)\n",
    "reca = recall_score(valid_y, preds)\n",
    "roc = roc_auc_score(valid_y, preds)\n",
    "f1 = f1_score(valid_y, preds)\n",
    "acc = accuracy_score(valid_y, preds)\n",
    "conf_mat = confusion_matrix(valid_y, preds)\n",
    "\n",
    "print(\"Precision: \", prec)\n",
    "print(\"Recall: \", reca)\n",
    "print(\"ROC score: \", roc)\n",
    "print(\"F1 score: \", f1)\n",
    "print(\"Accuracy score: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try ExtraTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.7914396003633061\n",
      "Recall:  0.8003444316877153\n",
      "ROC score:  0.8962929564719724\n",
      "F1 score:  0.7958671081173649\n",
      "Accuracy score:  0.9854327404860641\n"
     ]
    }
   ],
   "source": [
    "clf = estimator_dict['ExtraTreeClassifier'](random_state=42)\n",
    "\n",
    "clf.fit(train_X, np.ravel(train_y))\n",
    "\n",
    "preds = pd.DataFrame(clf.predict(valid_X), columns=['predictions'])\n",
    "\n",
    "prec = precision_score(valid_y, preds)\n",
    "reca = recall_score(valid_y, preds)\n",
    "roc = roc_auc_score(valid_y, preds)\n",
    "f1 = f1_score(valid_y, preds)\n",
    "acc = accuracy_score(valid_y, preds)\n",
    "conf_mat = confusion_matrix(valid_y, preds)\n",
    "\n",
    "print(\"Precision: \", prec)\n",
    "print(\"Recall: \", reca)\n",
    "print(\"ROC score: \", roc)\n",
    "print(\"F1 score: \", f1)\n",
    "print(\"Accuracy score: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.9739884393063584\n",
      "Recall:  0.8512055109070035\n",
      "ROC score:  0.9251846316888674\n",
      "F1 score:  0.9084670996201446\n",
      "Accuracy score:  0.9939140140621309\n"
     ]
    }
   ],
   "source": [
    "clf = estimator_dict['ExtraTreesClassifier'](random_state=42)\n",
    "\n",
    "clf.fit(train_X, np.ravel(train_y))\n",
    "\n",
    "preds = pd.DataFrame(clf.predict(valid_X), columns=['predictions'])\n",
    "\n",
    "prec = precision_score(valid_y, preds)\n",
    "reca = recall_score(valid_y, preds)\n",
    "roc = roc_auc_score(valid_y, preds)\n",
    "f1 = f1_score(valid_y, preds)\n",
    "acc = accuracy_score(valid_y, preds)\n",
    "conf_mat = confusion_matrix(valid_y, preds)\n",
    "\n",
    "print(\"Precision: \", prec)\n",
    "print(\"Recall: \", reca)\n",
    "print(\"ROC score: \", roc)\n",
    "print(\"F1 score: \", f1)\n",
    "print(\"Accuracy score: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eikegermann/anaconda3/envs/earthquake37/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.0\n",
      "Recall:  0.0\n",
      "ROC score:  0.5\n",
      "F1 score:  0.0\n",
      "Accuracy score:  0.9645187834545914\n"
     ]
    }
   ],
   "source": [
    "clf = estimator_dict['GaussianMixture'](random_state=42)\n",
    "\n",
    "clf.fit(train_X, np.ravel(train_y))\n",
    "\n",
    "preds = pd.DataFrame(clf.predict(valid_X), columns=['predictions'])\n",
    "\n",
    "prec = precision_score(valid_y, preds)\n",
    "reca = recall_score(valid_y, preds)\n",
    "roc = roc_auc_score(valid_y, preds)\n",
    "f1 = f1_score(valid_y, preds)\n",
    "acc = accuracy_score(valid_y, preds)\n",
    "conf_mat = confusion_matrix(valid_y, preds)\n",
    "\n",
    "print(\"Precision: \", prec)\n",
    "print(\"Recall: \", reca)\n",
    "print(\"ROC score: \", roc)\n",
    "print(\"F1 score: \", f1)\n",
    "print(\"Accuracy score: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.04088910187743136\n",
      "Recall:  0.9991963260619977\n",
      "ROC score:  0.5685062264844478\n",
      "F1 score:  0.07856323966166262\n",
      "Accuracy score:  0.16837894428104708\n"
     ]
    }
   ],
   "source": [
    "clf = estimator_dict['GaussianNB']()\n",
    "\n",
    "clf.fit(train_X, np.ravel(train_y))\n",
    "\n",
    "preds = pd.DataFrame(clf.predict(valid_X), columns=['predictions'])\n",
    "\n",
    "prec = precision_score(valid_y, preds)\n",
    "reca = recall_score(valid_y, preds)\n",
    "roc = roc_auc_score(valid_y, preds)\n",
    "f1 = f1_score(valid_y, preds)\n",
    "acc = accuracy_score(valid_y, preds)\n",
    "conf_mat = confusion_matrix(valid_y, preds)\n",
    "\n",
    "print(\"Precision: \", prec)\n",
    "print(\"Recall: \", reca)\n",
    "print(\"ROC score: \", roc)\n",
    "print(\"F1 score: \", f1)\n",
    "print(\"Accuracy score: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try GaussianProcessClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = estimator_dict['GaussianProcessClassifier'](random_state=42)\n",
    "\n",
    "# clf.fit(train_X, np.ravel(train_y))\n",
    "\n",
    "# preds = pd.DataFrame(clf.predict(valid_X), columns=['predictions'])\n",
    "\n",
    "# prec = precision_score(valid_y, preds)\n",
    "# reca = recall_score(valid_y, preds)\n",
    "# roc = roc_auc_score(valid_y, preds)\n",
    "# f1 = f1_score(valid_y, preds)\n",
    "# acc = accuracy_score(valid_y, preds)\n",
    "# conf_mat = confusion_matrix(valid_y, preds)\n",
    "\n",
    "# print(\"Precision: \", prec)\n",
    "# print(\"Recall: \", reca)\n",
    "# print(\"ROC score: \", roc)\n",
    "# print(\"F1 score: \", f1)\n",
    "# print(\"Accuracy score: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel dies when I run this one. Huh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.9057961359093938\n",
      "Recall:  0.7804822043628014\n",
      "ROC score:  0.8887481046985903\n",
      "F1 score:  0.8384828862164664\n",
      "Accuracy score:  0.9893311933257836\n"
     ]
    }
   ],
   "source": [
    "clf = estimator_dict['GradientBoostingClassifier'](random_state=42)\n",
    "\n",
    "clf.fit(train_X, np.ravel(train_y))\n",
    "\n",
    "preds = pd.DataFrame(clf.predict(valid_X), columns=['predictions'])\n",
    "\n",
    "prec = precision_score(valid_y, preds)\n",
    "reca = recall_score(valid_y, preds)\n",
    "roc = roc_auc_score(valid_y, preds)\n",
    "f1 = f1_score(valid_y, preds)\n",
    "acc = accuracy_score(valid_y, preds)\n",
    "conf_mat = confusion_matrix(valid_y, preds)\n",
    "\n",
    "print(\"Precision: \", prec)\n",
    "print(\"Recall: \", reca)\n",
    "print(\"ROC score: \", roc)\n",
    "print(\"F1 score: \", f1)\n",
    "print(\"Accuracy score: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try HistGradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.9213176916696291\n",
      "Recall:  0.8926521239954076\n",
      "ROC score:  0.9449238691708494\n",
      "F1 score:  0.9067584115691878\n",
      "Accuracy score:  0.9934862841267385\n"
     ]
    }
   ],
   "source": [
    "clf = estimator_dict['HistGradientBoostingClassifier'](random_state=42)\n",
    "\n",
    "clf.fit(train_X, np.ravel(train_y))\n",
    "\n",
    "preds = pd.DataFrame(clf.predict(valid_X), columns=['predictions'])\n",
    "\n",
    "prec = precision_score(valid_y, preds)\n",
    "reca = recall_score(valid_y, preds)\n",
    "roc = roc_auc_score(valid_y, preds)\n",
    "f1 = f1_score(valid_y, preds)\n",
    "acc = accuracy_score(valid_y, preds)\n",
    "conf_mat = confusion_matrix(valid_y, preds)\n",
    "\n",
    "print(\"Precision: \", prec)\n",
    "print(\"Recall: \", reca)\n",
    "print(\"ROC score: \", roc)\n",
    "print(\"F1 score: \", f1)\n",
    "print(\"Accuracy score: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = estimator_dict['KNeighborsClassifier']()\n",
    "\n",
    "clf.fit(train_X, np.ravel(train_y))\n",
    "\n",
    "preds = pd.DataFrame(clf.predict(valid_X), columns=['predictions'])\n",
    "\n",
    "prec = precision_score(valid_y, preds)\n",
    "reca = recall_score(valid_y, preds)\n",
    "roc = roc_auc_score(valid_y, preds)\n",
    "f1 = f1_score(valid_y, preds)\n",
    "acc = accuracy_score(valid_y, preds)\n",
    "conf_mat = confusion_matrix(valid_y, preds)\n",
    "\n",
    "print(\"Precision: \", prec)\n",
    "print(\"Recall: \", reca)\n",
    "print(\"ROC score: \", roc)\n",
    "print(\"F1 score: \", f1)\n",
    "print(\"Accuracy score: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takes several hours to run. Run overnight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.3548825822830719\n",
      "Recall:  0.44936854190585535\n",
      "ROC score:  0.709659267996497\n",
      "F1 score:  0.3965753077663509\n",
      "Accuracy score:  0.9514791308527712\n"
     ]
    }
   ],
   "source": [
    "clf = estimator_dict['LinearDiscriminantAnalysis']()\n",
    "\n",
    "clf.fit(train_X, np.ravel(train_y))\n",
    "\n",
    "preds = pd.DataFrame(clf.predict(valid_X), columns=['predictions'])\n",
    "\n",
    "prec = precision_score(valid_y, preds)\n",
    "reca = recall_score(valid_y, preds)\n",
    "roc = roc_auc_score(valid_y, preds)\n",
    "f1 = f1_score(valid_y, preds)\n",
    "acc = accuracy_score(valid_y, preds)\n",
    "conf_mat = confusion_matrix(valid_y, preds)\n",
    "\n",
    "print(\"Precision: \", prec)\n",
    "print(\"Recall: \", reca)\n",
    "print(\"ROC score: \", roc)\n",
    "print(\"F1 score: \", f1)\n",
    "print(\"Accuracy score: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eikegermann/anaconda3/envs/earthquake37/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.20697580441481034\n",
      "Recall:  0.9408725602755453\n",
      "ROC score:  0.9041298756642707\n",
      "F1 score:  0.3393093739648891\n",
      "Accuracy score:  0.8699945413513007\n"
     ]
    }
   ],
   "source": [
    "clf = estimator_dict['LogisticRegression'](random_state=42, class_weight='balanced', solver='saga', max_iter=1000)\n",
    "\n",
    "clf.fit(train_X, np.ravel(train_y))\n",
    "\n",
    "preds = pd.DataFrame(clf.predict(valid_X), columns=['predictions'])\n",
    "\n",
    "prec = precision_score(valid_y, preds)\n",
    "reca = recall_score(valid_y, preds)\n",
    "roc = roc_auc_score(valid_y, preds)\n",
    "f1 = f1_score(valid_y, preds)\n",
    "acc = accuracy_score(valid_y, preds)\n",
    "conf_mat = confusion_matrix(valid_y, preds)\n",
    "\n",
    "print(\"Precision: \", prec)\n",
    "print(\"Recall: \", reca)\n",
    "print(\"ROC score: \", roc)\n",
    "print(\"F1 score: \", f1)\n",
    "print(\"Accuracy score: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.8453395300800413\n",
      "Recall:  0.7517795637198622\n",
      "ROC score:  0.873359921910275\n",
      "F1 score:  0.795819154107924\n",
      "Accuracy score:  0.9863126420674428\n"
     ]
    }
   ],
   "source": [
    "clf = estimator_dict['MLPClassifier'](random_state=42)\n",
    "\n",
    "clf.fit(train_X, np.ravel(train_y))\n",
    "\n",
    "preds = pd.DataFrame(clf.predict(valid_X), columns=['predictions'])\n",
    "\n",
    "prec = precision_score(valid_y, preds)\n",
    "reca = recall_score(valid_y, preds)\n",
    "roc = roc_auc_score(valid_y, preds)\n",
    "f1 = f1_score(valid_y, preds)\n",
    "acc = accuracy_score(valid_y, preds)\n",
    "conf_mat = confusion_matrix(valid_y, preds)\n",
    "\n",
    "print(\"Precision: \", prec)\n",
    "print(\"Recall: \", reca)\n",
    "print(\"ROC score: \", roc)\n",
    "print(\"F1 score: \", f1)\n",
    "print(\"Accuracy score: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try NuSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eikegermann/anaconda3/envs/earthquake37/lib/python3.7/site-packages/sklearn/svm/_base.py:249: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.04318803939578578\n",
      "Recall:  0.5452353616532721\n",
      "ROC score:  0.5504376933281143\n",
      "F1 score:  0.08003640316505296\n",
      "Accuracy score:  0.5552708548895642\n"
     ]
    }
   ],
   "source": [
    "clf = estimator_dict['NuSVC'](nu=0.01, random_state=42, max_iter=10000)\n",
    "\n",
    "clf.fit(train_X, np.ravel(train_y))\n",
    "\n",
    "preds = pd.DataFrame(clf.predict(valid_X), columns=['predictions'])\n",
    "\n",
    "prec = precision_score(valid_y, preds)\n",
    "reca = recall_score(valid_y, preds)\n",
    "roc = roc_auc_score(valid_y, preds)\n",
    "f1 = f1_score(valid_y, preds)\n",
    "acc = accuracy_score(valid_y, preds)\n",
    "conf_mat = confusion_matrix(valid_y, preds)\n",
    "\n",
    "print(\"Precision: \", prec)\n",
    "print(\"Recall: \", reca)\n",
    "print(\"ROC score: \", roc)\n",
    "print(\"F1 score: \", f1)\n",
    "print(\"Accuracy score: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.9213176916696291\n",
      "Recall:  0.8926521239954076\n",
      "ROC score:  0.9449238691708494\n",
      "F1 score:  0.9067584115691878\n",
      "Accuracy score:  0.9934862841267385\n"
     ]
    }
   ],
   "source": [
    "clf = estimator_dict['OneVsRestClassifier'](estimator_dict['HistGradientBoostingClassifier'](random_state=42))\n",
    "\n",
    "clf.fit(train_X, np.ravel(train_y))\n",
    "\n",
    "preds = pd.DataFrame(clf.predict(valid_X), columns=['predictions'])\n",
    "\n",
    "prec = precision_score(valid_y, preds)\n",
    "reca = recall_score(valid_y, preds)\n",
    "roc = roc_auc_score(valid_y, preds)\n",
    "f1 = f1_score(valid_y, preds)\n",
    "acc = accuracy_score(valid_y, preds)\n",
    "conf_mat = confusion_matrix(valid_y, preds)\n",
    "\n",
    "print(\"Precision: \", prec)\n",
    "print(\"Recall: \", reca)\n",
    "print(\"ROC score: \", roc)\n",
    "print(\"F1 score: \", f1)\n",
    "print(\"Accuracy score: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eikegermann/anaconda3/envs/earthquake37/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:691: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.04084362284764949\n",
      "Recall:  0.9991963260619977\n",
      "ROC score:  0.5680057450085976\n",
      "F1 score:  0.07847928906042174\n",
      "Accuracy score:  0.16741349671258993\n"
     ]
    }
   ],
   "source": [
    "clf = estimator_dict['QuadraticDiscriminantAnalysis']()\n",
    "\n",
    "clf.fit(train_X, np.ravel(train_y))\n",
    "\n",
    "preds = pd.DataFrame(clf.predict(valid_X), columns=['predictions'])\n",
    "\n",
    "prec = precision_score(valid_y, preds)\n",
    "reca = recall_score(valid_y, preds)\n",
    "roc = roc_auc_score(valid_y, preds)\n",
    "f1 = f1_score(valid_y, preds)\n",
    "acc = accuracy_score(valid_y, preds)\n",
    "conf_mat = confusion_matrix(valid_y, preds)\n",
    "\n",
    "print(\"Precision: \", prec)\n",
    "print(\"Recall: \", reca)\n",
    "print(\"ROC score: \", roc)\n",
    "print(\"F1 score: \", f1)\n",
    "print(\"Accuracy score: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.9057961359093938\n",
      "Recall:  0.7804822043628014\n",
      "ROC score:  0.8887481046985903\n",
      "F1 score:  0.8384828862164664\n",
      "Accuracy score:  0.9893311933257836\n"
     ]
    }
   ],
   "source": [
    "clf = estimator_dict['RFE'](estimator_dict['GradientBoostingClassifier'](random_state=42))\n",
    "\n",
    "clf.fit(train_X, np.ravel(train_y))\n",
    "\n",
    "preds = pd.DataFrame(clf.predict(valid_X), columns=['predictions'])\n",
    "\n",
    "prec = precision_score(valid_y, preds)\n",
    "reca = recall_score(valid_y, preds)\n",
    "roc = roc_auc_score(valid_y, preds)\n",
    "f1 = f1_score(valid_y, preds)\n",
    "acc = accuracy_score(valid_y, preds)\n",
    "conf_mat = confusion_matrix(valid_y, preds)\n",
    "\n",
    "print(\"Precision: \", prec)\n",
    "print(\"Recall: \", reca)\n",
    "print(\"ROC score: \", roc)\n",
    "print(\"F1 score: \", f1)\n",
    "print(\"Accuracy score: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try RadiusNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = estimator_dict['RadiusNeighborsClassifier']()\n",
    "\n",
    "# clf.fit(train_X, np.ravel(train_y))\n",
    "\n",
    "# preds = pd.DataFrame(clf.predict(valid_X), columns=['predictions'])\n",
    "\n",
    "# prec = precision_score(valid_y, preds)\n",
    "# reca = recall_score(valid_y, preds)\n",
    "# roc = roc_auc_score(valid_y, preds)\n",
    "# f1 = f1_score(valid_y, preds)\n",
    "# acc = accuracy_score(valid_y, preds)\n",
    "# conf_mat = confusion_matrix(valid_y, preds)\n",
    "\n",
    "# print(\"Precision: \", prec)\n",
    "# print(\"Recall: \", reca)\n",
    "# print(\"ROC score: \", roc)\n",
    "# print(\"F1 score: \", f1)\n",
    "# print(\"Accuracy score: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm. Kerneldeath."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.1865723432358011\n",
      "Recall:  0.9568312284730195\n",
      "ROC score:  0.9016856799537398\n",
      "F1 score:  0.31225762940482205\n",
      "Accuracy score:  0.850453393731516\n"
     ]
    }
   ],
   "source": [
    "clf = estimator_dict['SGDClassifier'](loss='modified_huber', class_weight='balanced', average=False, random_state=42)\n",
    "\n",
    "clf.fit(train_X, np.ravel(train_y))\n",
    "\n",
    "preds = pd.DataFrame(clf.predict(valid_X), columns=['predictions'])\n",
    "\n",
    "prec = precision_score(valid_y, preds)\n",
    "reca = recall_score(valid_y, preds)\n",
    "roc = roc_auc_score(valid_y, preds)\n",
    "f1 = f1_score(valid_y, preds)\n",
    "acc = accuracy_score(valid_y, preds)\n",
    "conf_mat = confusion_matrix(valid_y, preds)\n",
    "\n",
    "print(\"Precision: \", prec)\n",
    "print(\"Recall: \", reca)\n",
    "print(\"ROC score: \", roc)\n",
    "print(\"F1 score: \", f1)\n",
    "print(\"Accuracy score: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = estimator_dict['SVC'](random_state=42)\n",
    "\n",
    "# clf.fit(train_X, np.ravel(train_y))\n",
    "\n",
    "# preds = pd.DataFrame(clf.predict(valid_X), columns=['predictions'])\n",
    "\n",
    "# prec = precision_score(valid_y, preds)\n",
    "# reca = recall_score(valid_y, preds)\n",
    "# roc = roc_auc_score(valid_y, preds)\n",
    "# f1 = f1_score(valid_y, preds)\n",
    "# acc = accuracy_score(valid_y, preds)\n",
    "# conf_mat = confusion_matrix(valid_y, preds)\n",
    "\n",
    "# print(\"Precision: \", prec)\n",
    "# print(\"Recall: \", reca)\n",
    "# print(\"ROC score: \", roc)\n",
    "# print(\"F1 score: \", f1)\n",
    "# print(\"Accuracy score: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah, nah. Dataset is a bit large for an SVM. (Says the guy who ran NuSVC anyway...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try StackingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.9009222661396574\n",
      "Recall:  0.7850746268656716\n",
      "ROC score:  0.8909492878216994\n",
      "F1 score:  0.8390184049079755\n",
      "Accuracy score:  0.989310825233622\n"
     ]
    }
   ],
   "source": [
    "namestring = 'esti_'\n",
    "estimator_list = [(namestring + str(num), estimator_dict['GradientBoostingClassifier'](random_state=42)) for num in range(5)]\n",
    "\n",
    "clf = estimator_dict['StackingClassifier'](estimator_list)\n",
    "\n",
    "clf.fit(train_X, np.ravel(train_y))\n",
    "\n",
    "preds = pd.DataFrame(clf.predict(valid_X), columns=['predictions'])\n",
    "\n",
    "prec = precision_score(valid_y, preds)\n",
    "reca = recall_score(valid_y, preds)\n",
    "roc = roc_auc_score(valid_y, preds)\n",
    "f1 = f1_score(valid_y, preds)\n",
    "acc = accuracy_score(valid_y, preds)\n",
    "conf_mat = confusion_matrix(valid_y, preds)\n",
    "\n",
    "print(\"Precision: \", prec)\n",
    "print(\"Recall: \", reca)\n",
    "print(\"ROC score: \", roc)\n",
    "print(\"F1 score: \", f1)\n",
    "print(\"Accuracy score: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.9057961359093938\n",
      "Recall:  0.7804822043628014\n",
      "ROC score:  0.8887481046985903\n",
      "F1 score:  0.8384828862164664\n",
      "Accuracy score:  0.9893311933257836\n"
     ]
    }
   ],
   "source": [
    "namestring = 'esti_'\n",
    "estimator_list = [(namestring + str(num), estimator_dict['GradientBoostingClassifier'](random_state=42)) for num in range(5)]\n",
    "\n",
    "clf = estimator_dict['VotingClassifier'](estimator_list)\n",
    "\n",
    "clf.fit(train_X, np.ravel(train_y))\n",
    "\n",
    "preds = pd.DataFrame(clf.predict(valid_X), columns=['predictions'])\n",
    "\n",
    "prec = precision_score(valid_y, preds)\n",
    "reca = recall_score(valid_y, preds)\n",
    "roc = roc_auc_score(valid_y, preds)\n",
    "f1 = f1_score(valid_y, preds)\n",
    "acc = accuracy_score(valid_y, preds)\n",
    "conf_mat = confusion_matrix(valid_y, preds)\n",
    "\n",
    "print(\"Precision: \", prec)\n",
    "print(\"Recall: \", reca)\n",
    "print(\"ROC score: \", roc)\n",
    "print(\"F1 score: \", f1)\n",
    "print(\"Accuracy score: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The voting classifier is obviously meant as an ensemble, so popping in multiple instances of the same algorithm isn't very useful. But this is a template to build on, it might be worth trying a whole bunch of combinations. Another rabbit hole to pop down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try XGBoost default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.9503260225251926\n",
      "Recall:  0.9203214695752009\n",
      "ROC score:  0.9592759173260763\n",
      "F1 score:  0.9350831146106736\n",
      "Accuracy score:  0.9954660626848404\n"
     ]
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "clf.fit(train_X, np.ravel(train_y))\n",
    "\n",
    "preds = pd.DataFrame(clf.predict(valid_X), columns=['predictions'])\n",
    "\n",
    "prec = precision_score(valid_y, preds)\n",
    "reca = recall_score(valid_y, preds)\n",
    "roc = roc_auc_score(valid_y, preds)\n",
    "f1 = f1_score(valid_y, preds)\n",
    "acc = accuracy_score(valid_y, preds)\n",
    "conf_mat = confusion_matrix(valid_y, preds)\n",
    "\n",
    "print(\"Precision: \", prec)\n",
    "print(\"Recall: \", reca)\n",
    "print(\"ROC score: \", roc)\n",
    "print(\"F1 score: \", f1)\n",
    "print(\"Accuracy score: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try XGBoost Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.9416342412451362\n",
      "Recall:  0.6668197474167623\n",
      "ROC score:  0.8326496486817733\n",
      "F1 score:  0.7807501008200027\n",
      "Accuracy score:  0.9867118566738091\n"
     ]
    }
   ],
   "source": [
    "clf = xgb.XGBRFClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "clf.fit(train_X, np.ravel(train_y))\n",
    "\n",
    "preds = pd.DataFrame(clf.predict(valid_X), columns=['predictions'])\n",
    "\n",
    "prec = precision_score(valid_y, preds)\n",
    "reca = recall_score(valid_y, preds)\n",
    "roc = roc_auc_score(valid_y, preds)\n",
    "f1 = f1_score(valid_y, preds)\n",
    "acc = accuracy_score(valid_y, preds)\n",
    "conf_mat = confusion_matrix(valid_y, preds)\n",
    "\n",
    "print(\"Precision: \", prec)\n",
    "print(\"Recall: \", reca)\n",
    "print(\"ROC score: \", roc)\n",
    "print(\"F1 score: \", f1)\n",
    "print(\"Accuracy score: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Light GBM Gradient Boosting Decision Tree\n",
    "\n",
    "NB: Light GBM supports categorical variables in numerical encoding. According to their documentation, categorical variables are faster than one-hot encoding, however, I left the preprocessing the same as with the rest to keep it comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.9244900393653823\n",
      "Recall:  0.8897818599311137\n",
      "ROC score:  0.9435542009604381\n",
      "F1 score:  0.9068039548353126\n",
      "Accuracy score:  0.9935107258373322\n"
     ]
    }
   ],
   "source": [
    "clf = lgb.LGBMClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "clf.fit(train_X, np.ravel(train_y))\n",
    "\n",
    "preds = pd.DataFrame(clf.predict(valid_X), columns=['predictions'])\n",
    "\n",
    "prec = precision_score(valid_y, preds)\n",
    "reca = recall_score(valid_y, preds)\n",
    "roc = roc_auc_score(valid_y, preds)\n",
    "f1 = f1_score(valid_y, preds)\n",
    "acc = accuracy_score(valid_y, preds)\n",
    "conf_mat = confusion_matrix(valid_y, preds)\n",
    "\n",
    "print(\"Precision: \", prec)\n",
    "print(\"Recall: \", reca)\n",
    "print(\"ROC score: \", roc)\n",
    "print(\"F1 score: \", f1)\n",
    "print(\"Accuracy score: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Light GBM Gradient-based One-Side Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.9208624570548514\n",
      "Recall:  0.892422502870264\n",
      "ROC score:  0.9448006116635373\n",
      "F1 score:  0.9064194507608887\n",
      "Accuracy score:  0.9934618424161445\n"
     ]
    }
   ],
   "source": [
    "clf = lgb.LGBMClassifier(boosting_type='goss', n_estimators=100, random_state=42)\n",
    "\n",
    "clf.fit(train_X, np.ravel(train_y))\n",
    "\n",
    "preds = pd.DataFrame(clf.predict(valid_X), columns=['predictions'])\n",
    "\n",
    "prec = precision_score(valid_y, preds)\n",
    "reca = recall_score(valid_y, preds)\n",
    "roc = roc_auc_score(valid_y, preds)\n",
    "f1 = f1_score(valid_y, preds)\n",
    "acc = accuracy_score(valid_y, preds)\n",
    "conf_mat = confusion_matrix(valid_y, preds)\n",
    "\n",
    "print(\"Precision: \", prec)\n",
    "print(\"Recall: \", reca)\n",
    "print(\"ROC score: \", roc)\n",
    "print(\"F1 score: \", f1)\n",
    "print(\"Accuracy score: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Light GBM Dropouts meet Multiple Additive Regression Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.9376590330788804\n",
      "Recall:  0.8461538461538461\n",
      "ROC score:  0.9220421723462624\n",
      "F1 score:  0.8895594447797225\n",
      "Accuracy score:  0.9925452782688751\n"
     ]
    }
   ],
   "source": [
    "clf = lgb.LGBMClassifier(boosting_type='dart', n_estimators=100, random_state=42)\n",
    "\n",
    "clf.fit(train_X, np.ravel(train_y))\n",
    "\n",
    "preds = pd.DataFrame(clf.predict(valid_X), columns=['predictions'])\n",
    "\n",
    "prec = precision_score(valid_y, preds)\n",
    "reca = recall_score(valid_y, preds)\n",
    "roc = roc_auc_score(valid_y, preds)\n",
    "f1 = f1_score(valid_y, preds)\n",
    "acc = accuracy_score(valid_y, preds)\n",
    "conf_mat = confusion_matrix(valid_y, preds)\n",
    "\n",
    "print(\"Precision: \", prec)\n",
    "print(\"Recall: \", reca)\n",
    "print(\"ROC score: \", roc)\n",
    "print(\"F1 score: \", f1)\n",
    "print(\"Accuracy score: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Light GBM Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.813812744749021\n",
      "Recall:  0.7873708381171067\n",
      "ROC score:  0.8903721049842541\n",
      "F1 score:  0.8003734609324853\n",
      "Accuracy score:  0.986064151343072\n"
     ]
    }
   ],
   "source": [
    "# RF type classifier requires variables bagging_freq > 0 and bagging_fraction in (0,1)\n",
    "clf = lgb.LGBMClassifier(boosting_type='rf', n_estimators=100, random_state=42, bagging_freq=1, bagging_fraction=0.9)\n",
    "\n",
    "clf.fit(train_X, np.ravel(train_y))\n",
    "\n",
    "preds = pd.DataFrame(clf.predict(valid_X), columns=['predictions'])\n",
    "\n",
    "prec = precision_score(valid_y, preds)\n",
    "reca = recall_score(valid_y, preds)\n",
    "roc = roc_auc_score(valid_y, preds)\n",
    "f1 = f1_score(valid_y, preds)\n",
    "acc = accuracy_score(valid_y, preds)\n",
    "conf_mat = confusion_matrix(valid_y, preds)\n",
    "\n",
    "print(\"Precision: \", prec)\n",
    "print(\"Recall: \", reca)\n",
    "print(\"ROC score: \", roc)\n",
    "print(\"F1 score: \", f1)\n",
    "print(\"Accuracy score: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Shogun ML AveragedPerceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shogun import (\n",
    "    AveragedPerceptron, GaussianKernel, LibSVM, RealFeatures, BinaryLabels, PrecisionMeasure\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = RealFeatures(train_X.to_numpy().transpose())\n",
    "features_test = RealFeatures(valid_X.to_numpy().transpose())\n",
    "labels_train = BinaryLabels((train_y.astype(int) * 2 - 1).to_numpy().reshape(-1))\n",
    "labels_test = BinaryLabels((valid_y.astype(int) * 2 - 1).to_numpy().reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_rate = 1.0\n",
    "max_iter = 1000\n",
    "perceptron = AveragedPerceptron(features_train, labels_train)\n",
    "perceptron.set_learn_rate(learn_rate)\n",
    "perceptron.set_max_iter(max_iter)\n",
    "perceptron.train()\n",
    "perceptron.set_features(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = perceptron.apply_binary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.DataFrame(predictions.get_values(), columns=['predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of binary and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-087ded0fb00b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mreca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mroc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/earthquake36/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/earthquake36/lib/python3.6/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1621\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'precision'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m                                                  \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1623\u001b[0;31m                                                  zero_division=zero_division)\n\u001b[0m\u001b[1;32m   1624\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/earthquake36/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/earthquake36/lib/python3.6/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >=0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m     labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n\u001b[0;32m-> 1434\u001b[0;31m                                     pos_label)\n\u001b[0m\u001b[1;32m   1435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m     \u001b[0;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/earthquake36/lib/python3.6/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1248\u001b[0m                          str(average_options))\n\u001b[1;32m   1249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1250\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1251\u001b[0m     \u001b[0mpresent_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'binary'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/earthquake36/lib/python3.6/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 91\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous targets"
     ]
    }
   ],
   "source": [
    "prec = precision_score(valid_y, preds)\n",
    "reca = recall_score(valid_y, preds)\n",
    "roc = roc_auc_score(valid_y, preds)\n",
    "f1 = f1_score(valid_y, preds)\n",
    "acc = accuracy_score(valid_y, preds)\n",
    "conf_mat = confusion_matrix(valid_y, preds)\n",
    "\n",
    "print(\"Precision: \", prec)\n",
    "print(\"Recall: \", reca)\n",
    "print(\"ROC score: \", roc)\n",
    "print(\"F1 score: \", f1)\n",
    "print(\"Accuracy score: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Shogun ML Kernel Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = RealFeatures(train_X.to_numpy().transpose())\n",
    "features_test = RealFeatures(valid_X.to_numpy().transpose())\n",
    "labels_train = BinaryLabels((train_y.astype(int) * 2 - 1).to_numpy().reshape(-1))\n",
    "labels_test = BinaryLabels((train_y.astype(int) * 2 - 1).to_numpy().reshape(-1))\n",
    "\n",
    "C = 1.0\n",
    "epsilon = 0.001\n",
    "gauss_kernel = GaussianKernel(features_train, features_train, 15)\n",
    "\n",
    "svm = LibSVM(C, gauss_kernel, labels_train)\n",
    "svm.set_epsilon(epsilon)\n",
    "svm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.DataFrame(svm.apply_binary(features_test).get_values(), columns=['predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of binary and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-087ded0fb00b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mreca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mroc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/earthquake36/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/earthquake36/lib/python3.6/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1621\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'precision'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m                                                  \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1623\u001b[0;31m                                                  zero_division=zero_division)\n\u001b[0m\u001b[1;32m   1624\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/earthquake36/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/earthquake36/lib/python3.6/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >=0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m     labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n\u001b[0;32m-> 1434\u001b[0;31m                                     pos_label)\n\u001b[0m\u001b[1;32m   1435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m     \u001b[0;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/earthquake36/lib/python3.6/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1248\u001b[0m                          str(average_options))\n\u001b[1;32m   1249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1250\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1251\u001b[0m     \u001b[0mpresent_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'binary'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/earthquake36/lib/python3.6/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 91\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous targets"
     ]
    }
   ],
   "source": [
    "prec = precision_score(valid_y, preds)\n",
    "reca = recall_score(valid_y, preds)\n",
    "roc = roc_auc_score(valid_y, preds)\n",
    "f1 = f1_score(valid_y, preds)\n",
    "acc = accuracy_score(valid_y, preds)\n",
    "conf_mat = confusion_matrix(valid_y, preds)\n",
    "\n",
    "print(\"Precision: \", prec)\n",
    "print(\"Recall: \", reca)\n",
    "print(\"ROC score: \", roc)\n",
    "print(\"F1 score: \", f1)\n",
    "print(\"Accuracy score: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, maybe I'm just a dunce, but this is not very cooperative."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
